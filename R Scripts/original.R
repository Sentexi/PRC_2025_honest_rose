â€ž# ===========================
# Fuel burn â€“ XGBoost + BayesOpt + Submission + Upload
# Version: ohne setTimeLimit, ohne Residual-Modelle
# ===========================

required_pkgs <- c(
Â Â "data.table","Matrix","ParBayesianOptimization",
Â Â "Metrics","doParallel","foreach","backports",
Â Â "arrow","tidyverse"
)
to_install <- setdiff(required_pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dependencies = TRUE)
invisible(lapply(required_pkgs, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE))))

library(xgboost)

`%||%` <- function(x, y) if (!is.null(x)) x else y

# =========================================================
# Helper: robust predict with best iteration
# =========================================================
pred_with_best <- function(model, dmat, best_iter = NULL) {
Â Â nt <- tryCatch(xgboost::xgb.num_trees(model), error = function(e) NA_integer_)
Â Â cand <- c(
Â Â Â Â suppressWarnings(as.integer(best_iter)),
Â Â Â Â suppressWarnings(as.integer(model$best_ntreelimit)),
Â Â Â Â suppressWarnings(as.integer(model$best_iteration)),
Â Â Â Â suppressWarnings(as.integer(nt))
Â Â )
Â Â bi <- cand[which(is.finite(cand) & !is.na(cand) & cand > 0L)][1]
Â Â if (!length(bi)) bi <- 1L
Â Â if (is.finite(nt) && !is.na(nt) && nt > 0L) {
Â Â Â Â bi <- max(1L, min(bi, nt))
Â Â } else {
Â Â Â Â bi <- max(1L, bi)
Â Â }
Â Â predict(model, dmat, ntreelimit = bi)
}

cv_with_timer <- function(params, data, folds, nrounds_cv, early_stop, label="cv") {
Â Â ptm <- proc.time()
Â Â cv <- xgboost::xgb.cv(
Â Â Â Â params = params, data = data,
Â Â Â Â nrounds = nrounds_cv, folds = folds,
Â Â Â Â early_stopping_rounds = early_stop, verbose = 0
Â Â )
Â Â elapsed <- (proc.time() - ptm)[["elapsed"]]
Â Â cat(sprintf("[CV %s] %.1fs | best=%.4f | best_iter=%d\n",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â label, elapsed,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â min(cv$evaluation_log$test_rmse_mean), cv$best_iteration))
Â Â cv
}

normalize_for_mm <- function(DT, feat_cols) {
Â Â for (col in feat_cols) {
Â Â Â Â v <- DT[[col]]
Â Â Â Â if (is.logical(v)) {
Â Â Â Â Â Â v <- as.integer(v); v[is.na(v)] <- 0L
Â Â Â Â } else if (is.numeric(v)) {
Â Â Â Â Â Â v[!is.finite(v)] <- NA_real_
Â Â Â Â } else {
Â Â Â Â Â Â v <- as.character(v)
Â Â Â Â Â Â v[is.na(v)] <- "__NA__"
Â Â Â Â Â Â v <- factor(v)
Â Â Â Â }
Â Â Â Â DT[[col]] <- v
Â Â }
Â Â DT
}

align_to_train <- function(X, refnames) {
Â Â miss <- setdiff(refnames, colnames(X))
Â Â if (length(miss)) {
Â Â Â Â X <- Matrix::cbind2(
Â Â Â Â Â Â X,
Â Â Â Â Â Â Matrix::Matrix(0, nrow(X), length(miss), sparse = TRUE,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dimnames = list(NULL, miss))
Â Â Â Â )
Â Â }
Â Â X <- X[, refnames, drop = FALSE]
Â Â X
}

set.seed(1337)

# =========================================================
# Load & basic filtering
# =========================================================
csv_path <- "C:/PRC Data Challenge 2025/features_intervals.csv"
stopifnot(file.exists(csv_path))

all_data <- read.csv(csv_path, stringsAsFactors = FALSE) |>
Â Â dplyr::mutate(
Â Â Â Â status = dplyr::case_when(
Â Â Â Â Â Â pct_elapsed_mid < 0Â Â Â ~ "taxi_out",
Â Â Â Â Â Â pct_elapsed_mid > 100 ~ "taxi_in",
Â Â Â Â Â Â TRUEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~ "inflight"
Â Â Â Â )
Â Â ) |>
Â Â dplyr::filter(!(status %in% c("taxi_in","taxi_out")))

dt <- data.table::as.data.table(all_data)
stopifnot("fuel_kg_min" %in% names(dt))
dt <- dt[!is.na(fuel_kg_min)]

# Drop komplett konstante oder komplett NA-Spalten
drop_cols_const <- names(dt)[vapply(dt, function(x) all(is.na(x)) || length(unique(x)) <= 1, logical(1))]
if (length(drop_cols_const)) dt[, (drop_cols_const) := NULL]

# =========================================================
# Split by flight_id, Feature-Set definieren
# =========================================================
dt_grp <- data.table::copy(dt)

drop_id_cols <- c(
Â Â "idx","flight_id","start","end","flight_date","takeoff","landed",
Â Â "start_hour_utc","end_hour_utc","midpoint_utc","model_time_utc",
Â Â "points_file_exists","origin_icao","dest_icao","dow","month",
Â Â "weather_code_text","precipitation","origin_region","dest_region",
Â Â "status"
)

keep_cols <- setdiff(names(dt_grp), drop_id_cols)
stopifnot("fuel_kg_min" %in% keep_cols)

all_flightsÂ Â <- unique(dt_grp$flight_id)
test_flights <- sample(all_flights, size = floor(0.20 * length(all_flights)))

train_dt <- dt_grp[!(flight_id %in% test_flights), ..keep_cols]
test_dtÂ Â <- dt_grp[ (flight_id %in% test_flights), ..keep_cols]

tr_fid <- dt_grp[!(flight_id %in% test_flights), flight_id]

# =========================================================
# Sparse Matrices
# =========================================================
library(Matrix)

tr <- data.table::copy(train_dt)
te <- data.table::copy(test_dt)

stopifnot("fuel_kg_min" %in% names(tr))
feat_cols <- setdiff(names(tr), "fuel_kg_min")

tr <- normalize_for_mm(tr, feat_cols)
te <- normalize_for_mm(te, feat_cols)

tt <- as.formula(paste("~ 0 +", paste(feat_cols, collapse = " + ")))
options(na.action = "na.pass")

X_train <- Matrix::sparse.model.matrix(tt, data = tr, na.action = stats::na.pass)
X_testÂ Â <- Matrix::sparse.model.matrix(tt, data = te, na.action = stats::na.pass)

y_train <- as.numeric(tr$fuel_kg_min)
y_testÂ Â <- as.numeric(te$fuel_kg_min)

cat("X_train rows:", nrow(X_train), "| y_train length:", length(y_train), "\n")
stopifnot(nrow(X_train) == length(y_train))

dtrain_full <- xgboost::xgb.DMatrix(data = X_train, label = y_train)
dtestÂ Â Â Â Â Â Â <- xgboost::xgb.DMatrix(data = X_test,Â Â label = y_test)

# =========================================================
# Folds (flight_id + optional type/duration OOS)
# =========================================================
K <- 5
groups <- unique(tr_fid)
fold_assign <- sample(rep(1:K, length.out = length(groups)))
folds <- lapply(1:K, function(k) {
Â Â gk <- groups[fold_assign == k]
Â Â which(tr_fid %in% gk)
})
folds_fid <- folds

make_group_folds <- function(groups, K=3) {
Â Â ug <- unique(groups)
Â Â assign <- sample(rep(1:K, length.out=length(ug)))
Â Â lapply(1:K, function(k) which(groups %in% ug[assign==k]))
}

# Type-OOS
if ("aircraft_type" %in% names(dt_grp)) {
Â Â tr_types <- dt_grp[!(flight_id %in% test_flights), aircraft_type]
Â Â folds_type <- make_group_folds(tr_types, K=3)
} else {
Â Â folds_type <- NULL
}

# Duration-OOS
if ("flight_duration_min" %in% names(dt_grp)) {
Â Â tr_dur <- dt_grp[!(flight_id %in% test_flights), flight_duration_min]
Â Â q <- quantile(tr_dur, probs = c(0,.25,.5,.75,1), na.rm=TRUE)
Â Â dur_bucket <- cut(tr_dur, breaks = unique(q), include.lowest = TRUE)
Â Â folds_dur <- make_group_folds(dur_bucket, K=3)
} else {
Â Â folds_dur <- NULL
}

# =========================================================
# Validation-Split fÃ¼r Final-Training
# =========================================================
n_tr <- nrow(X_train)
valid_idx <- sample.int(n_tr, size = max(1L, floor(0.10 * n_tr)))
dvalid <- xgboost::xgb.DMatrix(X_train[valid_idx, ],Â Â label = y_train[valid_idx])
dtrain <- xgboost::xgb.DMatrix(X_train[-valid_idx, ], label = y_train[-valid_idx])
watchlist <- list(train = dtrain, eval = dvalid)

# =========================================================
# Skip/Load-Logik
# =========================================================
LOAD_MODELÂ Â Â Â <- TRUEÂ Â Â # fertiges Modell laden?
LOAD_METADATA <- FALSEÂ Â # nur Meta laden?

model_path <- "C:/PRC Data Challenge 2025/xgb_fuel_burn_final_model.rds"
meta_pathÂ Â <- "C:/PRC Data Challenge 2025/xgb_fuel_burn_metadata.rds"

SKIP_BAYESOPTÂ Â Â Â Â Â <- FALSE
SKIP_FINAL_TRAINING <- FALSE

if (LOAD_MODEL && file.exists(model_path)) {
Â Â message("âš¡ Lade fertiges Modell: ", model_path)
Â Â final_model <- readRDS(model_path)
Â Â if (file.exists(meta_path)) {
Â Â Â Â meta <- readRDS(meta_path)
Â Â Â Â final_paramsÂ Â Â <- meta$params
Â Â Â Â X_train_namesÂ Â <- meta$feature_names
Â Â Â Â best_iterÂ Â Â Â Â Â <- meta$best_iteration
Â Â } else {
Â Â Â Â final_paramsÂ Â Â <- final_model$params
Â Â Â Â X_train_namesÂ Â <- final_model$feature_names
Â Â Â Â best_iterÂ Â Â Â Â Â <- final_model$best_iteration
Â Â }
Â Â SKIP_BAYESOPT <- TRUE
Â Â SKIP_FINAL_TRAINING <- TRUE
}

if (!LOAD_MODEL && LOAD_METADATA && file.exists(meta_path)) {
Â Â message("âš¡ Lade Metadaten (beste Hyperparameter): ", meta_path)
Â Â meta <- readRDS(meta_path)
Â Â final_paramsÂ Â <- meta$params
Â Â X_train_names <- meta$feature_names
Â Â best_iterÂ Â Â Â Â <- meta$best_iteration
Â Â 
Â Â if (is.numeric(final_params$grow_policy))
Â Â Â Â final_params$grow_policy <- if (final_params$grow_policy < 0.5) "depthwise" else "lossguide"
Â Â 
Â Â SKIP_BAYESOPTÂ Â Â Â Â Â <- TRUE
Â Â SKIP_FINAL_TRAINING <- FALSE
}

if (!LOAD_MODEL && !LOAD_METADATA) {
Â Â SKIP_BAYESOPTÂ Â Â Â Â Â <- FALSE
Â Â SKIP_FINAL_TRAINING <- FALSE
}

# =========================================================
# BayesOpt (nur wenn nÃ¶tig) â€“ ohne setTimeLimit!
# =========================================================
if (!SKIP_BAYESOPT) {
Â Â 
Â Â scorer <- function(eta, max_depth, min_child_weight, subsample,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â colsample_bytree, gamma, lambda, alpha,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â max_leaves, grow_policy) {
Â Â Â Â nrounds_cv <- 6000L; early_stop <- 60L
Â Â Â Â 
Â Â Â Â gp <- if (grow_policy < 0.5) "depthwise" else "lossguide"
Â Â Â Â params <- list(
Â Â Â Â Â Â objective="reg:squarederror", eval_metric="rmse",
Â Â Â Â Â Â device="cuda", tree_method="hist",
Â Â Â Â Â Â nthread=min(6L, max(2L, parallel::detectCores()-1L)),
Â Â Â Â Â Â single_precision_histogram=1, max_bin=256,
Â Â Â Â Â Â eta=eta, min_child_weight=min_child_weight, subsample=subsample,
Â Â Â Â Â Â colsample_bytree=colsample_bytree, gamma=gamma, lambda=lambda,
Â Â Â Â Â Â alpha=alpha, grow_policy=gp
Â Â Â Â )
Â Â Â Â if (gp=="depthwise") {
Â Â Â Â Â Â params$max_depthÂ Â <- as.integer(round(max_depth))
Â Â Â Â Â Â params$max_leaves <- 0L
Â Â Â Â } else {
Â Â Â Â Â Â params$max_depthÂ Â <- 0L
Â Â Â Â Â Â params$max_leaves <- max(16L, as.integer(round(max_leaves)))
Â Â Â Â }
Â Â Â Â 
Â Â Â Â rmse_list <- c(); best_iter <- 500L
Â Â Â Â 
Â Â Â Â out <- try({
Â Â Â Â Â Â cv_fid <- cv_with_timer(params, dtrain_full, folds_fid, nrounds_cv, early_stop, "fid"); gc()
Â Â Â Â Â Â rmse_list <- c(rmse_list, min(cv_fid$evaluation_log$test_rmse_mean))
Â Â Â Â Â Â best_iter <- cv_fid$best_iteration
Â Â Â Â Â Â 
Â Â Â Â Â Â if (!is.null(folds_type)) {
Â Â Â Â Â Â Â Â cv_type <- cv_with_timer(params, dtrain_full, folds_type, nrounds_cv, early_stop, "type"); gc()
Â Â Â Â Â Â Â Â rmse_list <- c(rmse_list, min(cv_type$evaluation_log$test_rmse_mean))
Â Â Â Â Â Â }
Â Â Â Â Â Â if (!is.null(folds_dur)) {
Â Â Â Â Â Â Â Â cv_dur <- cv_with_timer(params, dtrain_full, folds_dur, nrounds_cv, early_stop, "dur"); gc()
Â Â Â Â Â Â Â Â rmse_list <- c(rmse_list, min(cv_dur$evaluation_log$test_rmse_mean))
Â Â Â Â Â Â }
Â Â Â Â Â Â 
Â Â Â Â Â Â TRUE
Â Â Â Â }, silent = TRUE)
Â Â Â Â 
Â Â Â Â if (!isTRUE(out)) {
Â Â Â Â Â Â cat("âš ï¸ scorer timeout/err â†’ penalize point\n")
Â Â Â Â Â Â return(list(Score = -1e9, nrounds = best_iter))
Â Â Â Â }
Â Â Â Â 
Â Â Â Â rmse_meanÂ Â <- mean(rmse_list)
Â Â Â Â rmse_worst <- max(rmse_list)
Â Â Â Â list(Score = -(rmse_mean + 0.5 * rmse_worst), nrounds = best_iter)
Â Â }
Â Â 
Â Â bounds <- list(
Â Â Â Â etaÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â = c(0.01, 0.5),
Â Â Â Â max_depthÂ Â Â Â Â Â Â Â Â = c(8L, 32L),
Â Â Â Â min_child_weightÂ Â = c(4, 128),
Â Â Â Â subsampleÂ Â Â Â Â Â Â Â Â = c(0.4, 1.0),
Â Â Â Â colsample_bytreeÂ Â = c(0.4, 1.0),
Â Â Â Â gammaÂ Â Â Â Â Â Â Â Â Â Â Â Â = c(0.0, 15.0),
Â Â Â Â lambdaÂ Â Â Â Â Â Â Â Â Â Â Â = c(0.0, 15.0),
Â Â Â Â alphaÂ Â Â Â Â Â Â Â Â Â Â Â Â = c(0.0, 8.0),
Â Â Â Â max_leavesÂ Â Â Â Â Â Â Â = c(32L, 128L),
Â Â Â Â grow_policyÂ Â Â Â Â Â Â = c(0, 1.0)
Â Â )
Â Â 
Â Â opt <- ParBayesianOptimization::bayesOpt(
Â Â Â Â FUN = scorer, bounds = bounds,
Â Â Â Â initPoints = 25, iters.n = 25,
Â Â Â Â acq = "ei", parallel = FALSE,
Â Â Â Â gsPoints = 200L, plotProgress = FALSE, verbose = 1
Â Â )
Â Â best <- ParBayesianOptimization::getBestPars(opt)
Â Â print(best)
Â Â 
Â Â final_grow_policy <- if (best$grow_policy < 0.5) "depthwise" else "lossguide"
Â Â final_params <- list(
Â Â Â Â objective="reg:squarederror", eval_metric="rmse",
Â Â Â Â device="cuda", tree_method="hist",
Â Â Â Â nthread=min(6L, max(2L, parallel::detectCores()-1L)),
Â Â Â Â single_precision_histogram=1, max_bin=256,
Â Â Â Â eta=best$eta, min_child_weight=best$min_child_weight,
Â Â Â Â subsample=best$subsample, colsample_bytree=best$colsample_bytree,
Â Â Â Â gamma=best$gamma, lambda=best$lambda, alpha=best$alpha,
Â Â Â Â grow_policy=final_grow_policy
Â Â )
Â Â if (final_grow_policy=="depthwise") {
Â Â Â Â final_params$max_depthÂ Â <- as.integer(round(best$max_depth))
Â Â Â Â final_params$max_leaves <- 0L
Â Â } else {
Â Â Â Â final_params$max_depthÂ Â <- 0L
Â Â Â Â final_params$max_leaves <- max(16L, as.integer(round(best$max_leaves)))
Â Â }
}

# Safety: Meta geladen, aber kein opt
if (LOAD_METADATA && exists("final_params") && !exists("opt")) {
Â Â message("â„¹ï¸ Loaded metadata, proceeding to final training.")
} else if (LOAD_METADATA && !exists("final_params")) {
Â Â stop("âŒ Keine final_params in Metadaten gefunden â€“ Ã¼berprÃ¼fe ", meta_path)
}

# =========================================================
# Final Training (falls nÃ¶tig)
# =========================================================
if (!SKIP_FINAL_TRAINING) {
Â Â message("ðŸ Starte Final-Training mit Early Stopping â€¦")
Â Â 
Â Â if (exists("opt") && !is.null(opt$scoreSummary)) {
Â Â Â Â ss <- opt$scoreSummary
Â Â Â Â nbest <- ss$nrounds[which.max(ss$Value)]
Â Â Â Â nrounds_final <- max(500L, as.integer(round(nbest)) + 400L)
Â Â } else {
Â Â Â Â nrounds_final <- 2000L
Â Â }
Â Â 
Â Â final_model <- xgboost::xgb.train(
Â Â Â Â params = final_params,
Â Â Â Â dataÂ Â Â = dtrain,
Â Â Â Â nrounds = nrounds_final,
Â Â Â Â watchlist = watchlist,
Â Â Â Â early_stopping_rounds = 25,
Â Â Â Â verbose = 1
Â Â )
Â Â 
Â Â cat(sprintf("\nBest iteration (final fit): %d | eval-RMSE = %.6f\n",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_model$best_iteration,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_model$evaluation_log[final_model$best_iteration,]$eval_rmse))
}

# =========================================================
# Evaluation (Test RMSE, kg/min & kg/interval, Ensemble)
# =========================================================
if (!exists("X_train_names")) X_train_names <- colnames(X_train)
if (!exists("best_iter") || is.null(best_iter)) best_iter <- final_model$best_iteration

# Validation RMSE
val_rmse <- final_model$evaluation_log[final_model$best_iteration,]$eval_rmse
val_mean <- mean(y_train)
val_rmse_pct <- (val_rmse / pmax(val_mean, 1e-6)) * 100
cat(sprintf("\nValidation RMSE: %.2f (%.2f%% of mean fuel_kg_min)\n",
Â Â Â Â Â Â Â Â Â Â Â Â val_rmse, val_rmse_pct))

# Test-RMSE (kg/min)
X_test <- Matrix::sparse.model.matrix(tt, data = te, na.action = stats::na.pass)
X_test <- align_to_train(X_test, colnames(X_train))
cat("X_test rows:", nrow(X_test), "| y_test length:", length(y_test), "\n")
stopifnot(nrow(X_test) == length(y_test))

dtest <- xgboost::xgb.DMatrix(X_test, label = y_test)
pred_test <- pred_with_best(final_model, dtest, final_model$best_iteration)

rmse_test <- Metrics::rmse(y_test, pred_test)
eps_meanÂ Â <- max(mean(y_test, na.rm = TRUE), 1e-6)
rmse_test_pct <- (rmse_test / eps_mean) * 100
cat(sprintf("\nTEST RMSE (kg/min): %.2f (%.2f%% of mean)\n",
Â Â Â Â Â Â Â Â Â Â Â Â rmse_test, rmse_test_pct))

# Test-RMSE auf kg / Intervall
test_rows_dtgrp <- dt_grp[(flight_id %in% test_flights)]
interval_len_test <- as.numeric(test_rows_dtgrp$interval_min)
stopifnot(length(interval_len_test) == length(y_test))

y_test_totalÂ Â Â Â <- y_test * interval_len_test
pred_test_total <- as.numeric(pred_test) * interval_len_test

rmse_test_total <- Metrics::rmse(y_test_total, pred_test_total)
rmse_total_pctÂ Â <- (rmse_test_total / pmax(mean(y_test_total, na.rm = TRUE), 1e-6)) * 100

cat(sprintf("\nTEST RMSE (fuel per interval, single model): %.2f kg (%.2f%% of mean interval fuel)\n",
Â Â Â Â Â Â Â Â Â Â Â Â rmse_test_total, rmse_total_pct))

# Ensemble Ã¼ber Seeds
M <- 10L
seeds <- 2001:(2000+M)
pred_mat <- matrix(NA_real_, nrow = length(y_test), ncol = M)
best_iters <- integer(M)

for (j in seq_len(M)) {
Â Â cat(sprintf("\nEnsemble Model %d / %d\n", j, M))
Â Â set.seed(seeds[j])
Â Â 
Â Â params_j <- modifyList(final_params, list(
Â Â Â Â subsample = min(0.9, final_params$subsample),
Â Â Â Â colsample_bytree = min(0.9, final_params$colsample_bytree)
Â Â ))
Â Â 
Â Â bst_j <- xgboost::xgb.train(
Â Â Â Â paramsÂ Â Â = params_j,
Â Â Â Â dataÂ Â Â Â Â = dtrain,
Â Â Â Â nroundsÂ Â = nrounds_final,
Â Â Â Â watchlist = watchlist,
Â Â Â Â early_stopping_rounds = 50,
Â Â Â Â verbose = 0
Â Â )
Â Â 
Â Â best_iters[j] <- bst_j$best_ntreelimit %||%
Â Â Â Â bst_j$best_iteration %||%
Â Â Â Â xgboost::xgb.num_trees(bst_j)
Â Â 
Â Â pred_mat[, j] <- pred_with_best(bst_j, dtest, best_iters[j])
Â Â gc()
}

pred_ens <- rowMeans(pred_mat)
rmse_ens <- Metrics::rmse(y_test, pred_ens)
rmse_ens_pct <- 100 * rmse_ens / pmax(mean(y_test), 1e-6)
cat(sprintf("\nEnsemble TEST RMSE (kg/min): %.3f (%.2f%% of mean)\n",
Â Â Â Â Â Â Â Â Â Â Â Â rmse_ens, rmse_ens_pct))

# Ensemble auf kg / Intervall
interval_len_test <- dt_grp[(flight_id %in% test_flights), interval_min]
stopifnot(length(interval_len_test) == length(y_test))

y_totalÂ Â Â Â <- y_test * interval_len_test
pred_total <- as.numeric(pred_ens) * interval_len_test

ok <- is.finite(y_total) & is.finite(pred_total)
rmse_totalÂ Â Â Â Â <- sqrt(mean( (pred_total[ok] - y_total[ok])^2 ))
rmse_total_pct <- 100 * rmse_total / pmax(mean(y_total[ok]), 1e-6)

cat(sprintf("Ensemble TEST RMSE (interval kg): %.3f (%.2f%% of mean)\n",
Â Â Â Â Â Â Â Â Â Â Â Â rmse_total, rmse_total_pct))

# Importance & Save
imp <- xgboost::xgb.importance(model = final_model, feature_names = colnames(X_train))
print(utils::head(imp, 20))
write.csv(imp, "C:/PRC Data Challenge 2025/importance_matrix.csv", row.names = FALSE)

saveRDS(final_model, file = "C:/PRC Data Challenge 2025/xgb_fuel_burn_final_model.rds")
saveRDS(list(
Â Â feature_names = colnames(X_train),
Â Â params = final_params,
Â Â best_iteration = final_model$best_iteration,
Â Â test_rmse = rmse_total
), file = "C:/PRC Data Challenge 2025/xgb_fuel_burn_metadata.rds")

# VollstÃ¤ndiges Fehler-Feature-Set
stopifnot(length(y_test) == nrow(test_dt))
interval_len_test <- dt_grp[(flight_id %in% test_flights), interval_min]
stopifnot(length(interval_len_test) == nrow(test_dt))

actual_kgÂ Â Â Â <- y_test * interval_len_test
predicted_kg <- pred_ens * interval_len_test
abs_error <- abs(predicted_kg - actual_kg)
pct_error <- 100 * abs_error / pmax(actual_kg, 1e-6)

pred_df <- data.table::data.table(
Â Â flight_idÂ Â Â Â = dt_grp[(flight_id %in% test_flights), flight_id],
Â Â actual_kgÂ Â Â Â = actual_kg,
Â Â predicted_kg = predicted_kg,
Â Â abs_errorÂ Â Â Â = abs_error,
Â Â pct_errorÂ Â Â Â = pct_error
)

dt_test_full <- dt_grp[(flight_id %in% test_flights)]
stopifnot(nrow(dt_test_full) == nrow(pred_df))

df_full <- cbind(dt_test_full, pred_df[, .(actual_kg, predicted_kg, abs_error, pct_error)])
df_full <- df_full[order(-abs_error)]

out_err_path <- "C:/PRC Data Challenge 2025/predicted_vs_actual_full.csv"
data.table::fwrite(df_full, out_err_path)
cat(sprintf(
Â Â "\nâœ… VollstÃ¤ndiges Fehler-Feature-Set gespeichert unter:\n%s\n(%d Zeilen, %d Spalten)\n",
Â Â out_err_path, nrow(df_full), ncol(df_full)
))
print(head(df_full[, .(idx, flight_id, actual_kg, predicted_kg, abs_error, pct_error)], 10))

# =========================================================
# Submission (mit Taxi-Preds)
# =========================================================
X_train_names <- final_model$feature_names
final_paramsÂ Â <- final_model$params
best_iterÂ Â Â Â Â <- final_model$best_iteration

sub_path_inÂ Â <- "C:/PRC Data Challenge 2025/submission_intervals.csv"
stopifnot(file.exists(sub_path_in))

submission_df <- data.table::fread(sub_path_in) |>
Â Â dplyr::mutate(
Â Â Â Â status = dplyr::case_when(
Â Â Â Â Â Â pct_elapsed_mid < 0Â Â Â ~ "taxi_out",
Â Â Â Â Â Â pct_elapsed_mid > 100 ~ "taxi_in",
Â Â Â Â Â Â TRUEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ~ "inflight"
Â Â Â Â )
Â Â )

cat("Rows in submission:", nrow(submission_df), "\n")

only_taxi <- submission_df |>
Â Â dplyr::filter(status %in% c("taxi_out","taxi_in")) |>
Â Â dplyr::select(idx, flight_id, start, end)

submission_df <- submission_df |>
Â Â dplyr::filter(status == "inflight")

taxi_pred <- read.csv("C:/PRC Data Challenge 2025/submission_intervals_v4_scored.csv") |>
Â Â dplyr::select(-fuel_kg_min, -fuel_kg) |>
Â Â dplyr::rename(fuel_kg = fuel_kg_pred) |>
Â Â dplyr::select(idx, fuel_kg) |>
Â Â dplyr::left_join(only_taxi, by = "idx") |>
Â Â dplyr::select(idx, flight_id, start, end, fuel_kg)

# gleiche Normalisierung wie Training
feat_cols_sub <- setdiff(colnames(submission_df), c("fuel_kg_min"))
sub_for_mm <- normalize_for_mm(data.table::copy(submission_df), feat_cols_sub)

# alle Variablen im tt sicherstellen
need_vars <- setdiff(all.vars(tt), colnames(sub_for_mm))
if (length(need_vars)) {
Â Â for (v in need_vars) sub_for_mm[[v]] <- NA
}

X_sub <- Matrix::sparse.model.matrix(tt, data = sub_for_mm, na.action = stats::na.pass)
X_sub <- align_to_train(X_sub, X_train_names)
stopifnot(identical(colnames(X_train), colnames(X_sub)))
dsubÂ Â <- xgboost::xgb.DMatrix(X_sub)

pred_sub_min <- pred_with_best(final_model, dsub, final_model$best_iteration)

submission_df$fuel_kg <- pred_sub_min * submission_df$interval_min
stopifnot(nrow(submission_df) == nrow(X_sub))
stopifnot(!any(is.na(submission_df$fuel_kg)))

qs <- quantile(pred_sub_min, c(0,.01,.5,.99,1), na.rm=TRUE)
cat("pred_sub_min quantiles:", paste(round(qs,3), collapse=" | "), "\n")

submission_df <- submission_df |>
Â Â dplyr::select(idx, flight_id, start, end, fuel_kg)

submission_df <- dplyr::bind_rows(submission_df, taxi_pred)

out_df <- submission_df[, c("idx","flight_id","start","end","fuel_kg")]
out_path <- "C:/PRC Data Challenge 2025/honest-rose_v20.parquet"

arrow::write_parquet(out_df, out_path)
cat(sprintf("âœ… Parquet geschrieben: %s | Zeilen: %d\n", out_path, nrow(out_df)))

# =========================================================
# Upload zu OpenSky S3 (MinIO)
# =========================================================
mc_bin <- "mc.exe"
if (.Platform$OS.type != "windows") mc_bin <- "mc"

local_fileÂ Â <- "C:/PRC Data Challenge 2025/honest-rose_v20.parquet"
target_path <- "opensky/prc-2025-honest-rose/honest-rose_v20.parquet"

cat("Richte mc alias 'opensky' ein...\n")
alias_cmd <- c("alias", "set", "--api", "S3v4", "--path", "auto",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "opensky", "https://s3.opensky-network.org",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "3tdiGZNiuaKj9I7S", "tb1RouZ1LHRYU3ZUIMy5TFGzj4sSYgTB")
system2(mc_bin, args = alias_cmd, stdout = TRUE, stderr = TRUE)

cat(sprintf("Lade hoch: %s â†’ %s\n", local_file, target_path))
res_cp <- tryCatch(
Â Â system2(mc_bin, args = c("cp", shQuote(local_file), shQuote(target_path)),
Â Â Â Â Â Â Â Â Â Â stdout = TRUE, stderr = TRUE),
Â Â error = function(e) e
)

if (inherits(res_cp, "error")) {
Â Â cat("âŒ Upload FEHLGESCHLAGEN:\n")
Â Â print(res_cp)
} else {
Â Â cat("âœ… Upload ausgefÃ¼hrt, prÃ¼fe Sichtbarkeit...\n")
Â Â res_ls <- tryCatch(
Â Â Â Â system2(mc_bin, args = c("ls", "opensky/prc-2025-honest-rose/"), stdout = TRUE),
Â Â Â Â error = function(e) e
Â Â )
Â Â if (inherits(res_ls, "error")) {
Â Â Â Â cat("âš ï¸ Verifikation fehlgeschlagen:\n")
Â Â Â Â print(res_ls)
Â Â } else {
Â Â Â Â if (any(grepl("honest-rose_v20.parquet", res_ls))) {
Â Â Â Â Â Â cat("ðŸŽ‰ Datei erfolgreich im Bucket sichtbar!\n")
Â Â Â Â } else {
Â Â Â Â Â Â cat("âš ï¸ Upload ausgefÃ¼hrt, aber Datei nicht gelistet â€” evtl. Cache-VerzÃ¶gerung.\n")
Â Â Â Â }
Â Â }
}â€œ
